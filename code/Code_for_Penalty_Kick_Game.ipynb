{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ATz1Adnc_-tP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9330d06-8eaf-4bed-e091-2c03f6cd9eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting quantecon\n",
            "  Downloading quantecon-0.7.0-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.8/214.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.49.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from quantecon) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.10.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.11.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49.0->quantecon) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49.0->quantecon) (67.7.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->quantecon) (1.3.0)\n",
            "Installing collected packages: quantecon\n",
            "Successfully installed quantecon-0.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nashpy\n",
            "  Downloading nashpy-0.0.37-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from nashpy) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from nashpy) (1.10.1)\n",
            "Requirement already satisfied: networkx>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nashpy) (3.1)\n",
            "Installing collected packages: nashpy\n",
            "Successfully installed nashpy-0.0.37\n"
          ]
        }
      ],
      "source": [
        "! pip install quantecon\n",
        "! pip install nashpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tian Ji's Horse Racing Game and Strategy\n",
        "\n",
        "Tian Ji's Horse Racing is a game that follows the following rule: Tian Ji and the king of the Qi Kingdom are the two players in this game. The king of possesses superior horses than Tian Ji. In each round of the game, Tian Ji and the king both select three horses, categorizing them as good, better, and best. The race operates under the rule of three rounds, with the victor being the one who wins at least two rounds. Here is the game matrix for Tian Ji.\n",
        "\n",
        "|            | ABC   | ACB   | BAC   | BCA   | CAB   | CBA   |\n",
        "|------------|-------|-------|-------|-------|-------|-------|\n",
        "| ABC        | -1, 1 | -1, 1 | -1, 1 | 1, -1 | -1, 1 | -1, 1 |\n",
        "| ACB        | -1, 1 | -1, 1 | 1, -1 | -1, 1 | -1, 1 | -1, 1 |\n",
        "| BAC        | -1, 1 | -1, 1 | -1, 1 | -1, 1 | -1, 1 | 1, -1 |\n",
        "| BCA        | -1, 1 | -1, 1 | -1, 1 | -1, 1 | 1, -1 | -1, 1 |\n",
        "| CAB        | 1, -1 | -1, 1 | -1, 1 | -1, 1 | -1, 1 | -1, 1 |\n",
        "| CBA        | -1, 1 | 1, -1 | -1, 1 | -1, 1 | -1, 1 | -1, 1 |\n",
        "\n",
        "Tian Ji (T) and King (K) use the following strategy in horse racing:\n",
        "\n",
        "- T's \"good\" horse (C) races against K's \"best\" horse (A).\n",
        "- T's \"best\" horse (A) races against K's \"better\" horse (B).\n",
        "- T's \"better\" horse (B) races against K's \"good\" horse (C).\n",
        "\n",
        "This strategy allows Tian Ji to win the race by losing the first round but winning the second and third rounds.\n"
      ],
      "metadata": {
        "id": "o0FXBe8ZmBOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Penalty Kick Game Based on Horse Racing Game"
      ],
      "metadata": {
        "id": "nkghtPH-gI_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Game Description of The Penalty Game:\n",
        "The Penalty kick game simulates the penalty shoot-out in real football games. This game combines the real world football senarios and the Tian Ji's horse racing game. This game follows the following rule: two coaches in two different football teams are the two players in this game. One team's player is better than the other team. In each game, two coaches both select three players. The player has different levels, which are low(L), middle(M), and high(H). The game operates under the rule of three rounds, with the victor being the one who first wins two rounds. Here is the game matrix for the week team.\n",
        "\n",
        "|            | HML   | HLM   | MHL   | MLH   | LHM   | LMH   |\n",
        "|------------|-------|-------|-------|-------|-------|-------|\n",
        "| HML        | -1, 1 | -1, 1 | -1, 1 | 1, -1 | -1, 1 | -1, 1 |\n",
        "| HLM        | -1, 1 | -1, 1 | 1, -1 | -1, 1 | -1, 1 | -1, 1 |\n",
        "| MHL        | -1, 1 | -1, 1 | -1, 1 | -1, 1 | -1, 1 | 1, -1 |\n",
        "| MLH        | -1, 1 | -1, 1 | -1, 1 | -1, 1 | 1, -1 | -1, 1 |\n",
        "| LHM        | 1, -1 | -1, 1 | -1, 1 | -1, 1 | -1, 1 | -1, 1 |\n",
        "| LMH        | -1, 1 | 1, -1 | -1, 1 | -1, 1 | -1, 1 | -1, 1 |\n"
      ],
      "metadata": {
        "id": "USOOV4CcAeXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Solution for The Penalty Game"
      ],
      "metadata": {
        "id": "mI_PvkLBxNun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Solution using Nash Equilibrium\n",
        "import nashpy as nash\n",
        "import numpy as np\n",
        "\n",
        "# Create the matrix\n",
        "A = np.array([[-1, -1, -1, 1, -1, -1], [-1, -1, 1, -1, -1, -1],[-1, -1, -1, -1, -1, 1],[-1, -1, -1, -1, 1, -1],[1, -1, -1, -1, -1, -1],[-1, 1, -1, -1, -1, -1]])\n",
        "B = np.array([[1, 1, 1, -1, 1, 1], [1, 1, -1, 1, 1, 1],[1, 1, 1, 1, 1, -1],[1, 1, 1, 1, -1, 1],[-1, 1, 1, 1, 1, 1],[1, -1, 1, 1, 1, 1]])\n",
        "\n",
        "# Create the game\n",
        "penalty = nash.Game(A, B)\n",
        "penalty\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQQEEyr5xM8_",
        "outputId": "f4fbc55e-13f1-42b6-8054-857e1098f6e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Zero sum game with payoff matrices:\n",
              "\n",
              "Row player:\n",
              "[[-1 -1 -1  1 -1 -1]\n",
              " [-1 -1  1 -1 -1 -1]\n",
              " [-1 -1 -1 -1 -1  1]\n",
              " [-1 -1 -1 -1  1 -1]\n",
              " [ 1 -1 -1 -1 -1 -1]\n",
              " [-1  1 -1 -1 -1 -1]]\n",
              "\n",
              "Column player:\n",
              "[[ 1  1  1 -1  1  1]\n",
              " [ 1  1 -1  1  1  1]\n",
              " [ 1  1  1  1  1 -1]\n",
              " [ 1  1  1  1 -1  1]\n",
              " [-1  1  1  1  1  1]\n",
              " [ 1 -1  1  1  1  1]]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the Nash equilibrium\n",
        "equilibria = penalty.support_enumeration()\n",
        "for eq in equilibria:\n",
        "    print(eq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkpyDK41_V-O",
        "outputId": "07bc800c-27ae-41d8-a3c3-b3a56afbaf77"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
            "       0.16666667]), array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
            "       0.16666667]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description of The Solution\n",
        "\n",
        "From the game description, the coach of the weak team and the coach of the strong team have different strategies to choose from. Let's analyze the week team coach's (W) strategy in response to the strong team coach's (S) choices:\n",
        "\n",
        "- If the S chooses HML, W can choose LHM.\n",
        "- If the S chooses HLM, W can choose LMH.\n",
        "- If the S chooses MHL, W can choose HLM.\n",
        "- If the S chooses MLH, W can choose HML.\n",
        "- If the S chooses LHM, W can choose MLH.\n",
        "- If the S chooses LMH, W can choose MHL.\n",
        "\n",
        "Based on these strategies, W can select the corresponding option that brings him the maximum benefit, depending on S's choice. It is important to note that none of these strategies are mutual best responses, and thus, there is no pure Nash equilibrium strategy in the game.\n",
        "\n",
        "From the code solution, a mixed Nash equilibrium strategy exists. If W does not know the strategy of the other team, both players can change their strategies. The dominant strategy for S is to choose randomly. Regardless of the strategy he chooses, the probability for S to win is 5/6. The expected payoff for S is 5/6. Similarly, the dominant strategy for W is also to choose randomly since the probability for W to win is 1/6. The expected payoff for W is -5/6. Therefore, in the absence of knowledge about the opponent's strategy, the dominant strategy for both Teams is to choose randomly."
      ],
      "metadata": {
        "id": "4bqau02gnPmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Penalty Kick Game With Shooting Probability\n",
        "## Game Description of The Penalty Game:\n",
        "In real-world scenarios, there is a possibility that the player misses the goal. Based on the situation, the game sets the probability of shooting as follows:\n",
        "- Week team: High-level(80%), Middle-level(60%), Low-Level(40%)\n",
        "- Strong team: High-level(90%), Middle-level(70%), Low-Level(50%)\n",
        "\n",
        "Thus, the game matrix is as follows:\n",
        "\n",
        "|            | HML   | HLM   | MHL   | MLH   | LHM   | LMH   |\n",
        "|------------|-------|-------|-------|-------|-------|-------|\n",
        "| HML        | -2.1, 2.1 | -1, 1 | -0.6, 0.6 | 0.5, -0.5 | -0.8, 0.8 | -0.8, 0.8 |\n",
        "| HLM        | -1, 1 | -2.1, 2.1 | 0.5, -0.5 | -0.6, 0.6 | -0.8, 0.8 | -0.8, 0.8 |\n",
        "| MHL        | -0.6, 0.6 | -0.8, 0.8 | -2.1, 2.1 | -0.8, 0.8 | -1, 1 | 0.5, -0.5 |\n",
        "| MLH        | -0.8, 0.8 | -0.6, 0.6 | -0.8, 0.8 | -2.1, 2.1 | 0.5, -0.5 | -1, 1 |\n",
        "| LHM        | 0.5, -0.5 | -0.8, 0.8 | -1, 1 | -0.8, 0.8 | -2.1, 2.1 | -0.6, 0.6 |\n",
        "| LMH        | -0.8, 0.8 | 0.5, -0.5 | -0.8, 0.8 | -1, 1 | -0.6, 0.6 | -2.1, 2.1 |"
      ],
      "metadata": {
        "id": "XXwr07OihAcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Coding Solution"
      ],
      "metadata": {
        "id": "b0zIfrbrgXVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Solution using Nash Equilibrium\n",
        "import nashpy as nash\n",
        "import numpy as np\n",
        "\n",
        "# Create the matrix\n",
        "A = np.array([[-2.1, -1, -0.6, 0.5, -0.8, -0.8], [-1, -2.1, 0.5, -0.6, -0.8, -0.8],[-0.6, -0.8, -2.1, -0.8, -1, 0.5],[-0.8, -0.6, -0.8, -2.1, 0.5, -1],[0.5, -0.8, -1, -0.8, -2.1, -0.6],[-0.8, 0.5, -0.8, -1, -0.6, -2.1]])\n",
        "B = np.array([[2.1, 1, 0.6, -0.5, 0.8, 0.8], [1, 2.1, -0.5, 0.6, 0.8, 0.8],[0.6, 0.8, 2.1, 0.8, 1, -0.5],[0.8, 0.6, 0.8, 2.1, -0.5, 1],[-0.5, 0.8, 1, 0.8, 2.1, 0.6],[0.8, -0.5, 0.8, 1, 0.6, 2.1]])\n",
        "\n",
        "# Create the game\n",
        "penalty_p = nash.Game(A, B)\n",
        "penalty_p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4d47UT8Lp6d",
        "outputId": "0465ab4f-f881-45fa-ae85-b3cb8ca1ee00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Zero sum game with payoff matrices:\n",
              "\n",
              "Row player:\n",
              "[[-2.1 -1.  -0.6  0.5 -0.8 -0.8]\n",
              " [-1.  -2.1  0.5 -0.6 -0.8 -0.8]\n",
              " [-0.6 -0.8 -2.1 -0.8 -1.   0.5]\n",
              " [-0.8 -0.6 -0.8 -2.1  0.5 -1. ]\n",
              " [ 0.5 -0.8 -1.  -0.8 -2.1 -0.6]\n",
              " [-0.8  0.5 -0.8 -1.  -0.6 -2.1]]\n",
              "\n",
              "Column player:\n",
              "[[ 2.1  1.   0.6 -0.5  0.8  0.8]\n",
              " [ 1.   2.1 -0.5  0.6  0.8  0.8]\n",
              " [ 0.6  0.8  2.1  0.8  1.  -0.5]\n",
              " [ 0.8  0.6  0.8  2.1 -0.5  1. ]\n",
              " [-0.5  0.8  1.   0.8  2.1  0.6]\n",
              " [ 0.8 -0.5  0.8  1.   0.6  2.1]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the Nash equilibrium\n",
        "equilibria = penalty_p.support_enumeration()\n",
        "for eq in equilibria:\n",
        "    print(eq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvIsLx7fVgSp",
        "outputId": "1ce843f0-f408-4494-b9df-548038c1dcee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0.        , 0.33333333, 0.33333333, 0.        , 0.        ,\n",
            "       0.33333333]), array([0.        , 0.33333333, 0.33333333, 0.        , 0.        ,\n",
            "       0.33333333]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description of The Solution\n",
        "\n",
        "Though the payoff has changed, the weak team coach's (W) strategy in response to the strong team coach's (S) choices is still the same, which is similar to Tian Ji's strategy. It is because the weak team has only one strategy out of six to win over the strong team. Besides, there is still no pure Nash equilibrium strategy in the game. The strategy is as follows:\n",
        "\n",
        "- If the S chooses HML, W can choose LHM.\n",
        "- If the S chooses HLM, W can choose LMH.\n",
        "- If the S chooses MHL, W can choose HLM.\n",
        "- If the S chooses MLH, W can choose HML.\n",
        "- If the S chooses LHM, W can choose MLH.\n",
        "- If the S chooses LMH, W can choose MHL.\n",
        "\n",
        "\n",
        "From the code solution, the mixed Nash equilibrium strategy has changed along with the change in the payoff. The dominant strategy for S is to choose randomly from HLM, MHL, and LMH. The probability for S to win is 2/3. The expected payoff is 7/10. The dominant strategy for W is also to choose randomly from HLM, MHL, and LMH. The probability for W to win is 1/3. The expected payoff is -7/10."
      ],
      "metadata": {
        "id": "7FRRql1Tp4nk"
      }
    }
  ]
}